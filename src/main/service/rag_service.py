"""
RAG Service - Complete implementation with upload and ask functions
"""

import sys
import os
from pathlib import Path

# Ajouter le r√©pertoire parent au PYTHONPATH
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent  # Remonte √† la racine du projet
sys.path.insert(0, str(project_root))

from src.main.service.database_vect_service.database_vect_service import DatabaseVectService
from src.main.service.llm_service.llm_service import LlmService
from src.main.service.chunk_service.chunk_service import ChunkService
from src.main.service.embedding_service.embedding_service import EmbeddingService
from src.main.model.config import Config
import json
import shutil


def load_file(file):
    """Load JSON configuration file"""
    path = f"{os.getcwd()}"
    with open(f"{path}/{file}", 'r', encoding='utf-8') as read_file:
        return json.load(read_file)


class RagService:
    """
    Service RAG principal avec deux fonctions principales:
    - upload(): charge un document, le chunke, l'embedde et le stocke
    - ask(): r√©pond √† une question en utilisant la base vectorielle
    """
    
    def __init__(self):
        # Load configuration
        self.config = Config(load_file("src/main/config/config.json"))
        
        # Initialize services
        self.chunk_service = ChunkService(self.config)
        self.embedding_service = EmbeddingService()
        self.database_vect_service = DatabaseVectService(self.config)
        self.llm_service = LlmService()
        
        # Default collection name (peut √™tre configur√©)
        self.collection_name = "documents"
        self.collection = self.database_vect_service.get_or_create_collection(
            self.collection_name
        )
        
        print("‚úì RAG Service initialized")
    
    def upload(
        self, 
        file_path: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        collection_name: str = None
    ) -> dict:
        """
        Upload un document depuis un chemin fichier, le convertit en chunks, 
        g√©n√®re les embeddings et stocke tout dans la base vectorielle.
        
        Args:
            file_path: Chemin vers le fichier (str ou Path)
            chunk_size: Taille des chunks en caract√®res
            chunk_overlap: Chevauchement entre chunks
            collection_name: Nom de la collection (optionnel)
        
        Returns:
            dict: R√©sum√© de l'upload avec statistiques
        """
        
        # Convertir en Path
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Fichier introuvable: {file_path}")
        
        print(f"\n{'='*60}")
        print(f"UPLOAD: {file_path.name}")
        print(f"{'='*60}")
        
        # Utilise la collection sp√©cifi√©e ou celle par d√©faut
        if collection_name:
            collection = self.database_vect_service.get_or_create_collection(
                collection_name
            )
        else:
            collection = self.collection
        
        try:
            # 1. Convertir en Markdown si n√©cessaire
            file_extension = file_path.suffix.lower()
            
            if file_extension in ['.pdf', '.docx', '.doc', '.xlsx', '.pptx']:
                print(f"‚Üí Conversion en Markdown ({file_extension})...")
                markdown_content = self._convert_to_markdown(file_path)
            elif file_extension == '.md':
                with open(file_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
            else:
                # Texte brut
                with open(file_path, 'r', encoding='utf-8') as f:
                    markdown_content = f.read()
            
            print(f"‚úì Document charg√© ({len(markdown_content)} caract√®res)")
            
            # 2. Chunker le document
            print(f"‚Üí Chunking (size={chunk_size}, overlap={chunk_overlap})...")
            chunks = self.chunk_service.chunk(
                pages=markdown_content,
                size=chunk_size,
                overlap=chunk_overlap
            )
            
            num_chunks = len(chunks)
            print(f"‚úì {num_chunks} chunks cr√©√©s")
            
            # 3. G√©n√©rer les embeddings
            print(f"‚Üí G√©n√©ration des embeddings...")
            
            # Extraire le texte des chunks
            chunk_texts = [chunk if isinstance(chunk, str) else chunk.page_content 
                          for chunk in chunks]
            
            # Embedder par batch
            embeddings = self.embedding_service.embed_batch(
                texts=chunk_texts,
                batch_size=32,
                is_query=False,
                show_progress=True
            )
            
            print(f"‚úì {len(embeddings)} embeddings g√©n√©r√©s")
            
            # 4. Stocker dans ChromaDB
            print(f"‚Üí Stockage dans ChromaDB...")
            
            # Obtenir le dernier ID
            all_ids = collection.get()["ids"]
            numeric_ids = [int(i) for i in all_ids if i.isdigit()]
            last_id = max(numeric_ids) if numeric_ids else 0
            
            # Stocker chaque chunk avec son embedding
            for i, (chunk_text, embedding) in enumerate(zip(chunk_texts, embeddings)):
                chunk_id = str(last_id + i + 1)
                
                # M√©tadonn√©es
                metadata = {
                    "source": file_path.name,
                    "source_path": str(file_path),
                    "chunk_index": i,
                    "total_chunks": num_chunks,
                    "file_type": file_extension
                }
                
                # Si c'est un objet Document de langchain
                if hasattr(chunks[i], 'metadata'):
                    metadata.update(chunks[i].metadata)
                
                # Stocker
                self.database_vect_service.collection_add_or_update(
                    collection=collection,
                    id=chunk_id,
                    embedding_vector=embedding,
                    documents=chunk_text,
                    metadatas=metadata
                )
            
            print(f"‚úì {num_chunks} chunks stock√©s dans '{collection.name}'")
            
            # R√©sum√©
            result = {
                "status": "success",
                "filename": file_path.name,
                "file_path": str(file_path),
                "file_type": file_extension,
                "num_chunks": num_chunks,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "collection": collection.name,
                "embedding_dimension": len(embeddings[0]),
                "total_characters": len(markdown_content)
            }
            
            print(f"\n{'='*60}")
            print("‚úì UPLOAD TERMIN√â")
            print(json.dumps(result, indent=2))
            print(f"{'='*60}\n")
            
            return result
        
        except Exception as e:
            print(f"‚úó ERREUR lors de l'upload: {e}")
            raise
    
    def ask(
        self,
        question: str,
        collection_name: str = None,
        top_k: int = 5
    ) -> dict:
        """
        R√©pond √† une question en utilisant le RAG.
        
        Args:
            question: La question de l'utilisateur
            collection_name: Collection √† interroger (optionnel)
            top_k: Nombre de documents similaires √† r√©cup√©rer
        
        Returns:
            dict: R√©ponse avec contexte et m√©tadonn√©es
        """
        
        print(f"\n{'='*60}")
        print(f"QUESTION: {question}")
        print(f"{'='*60}")
        
        # Utilise la collection sp√©cifi√©e ou celle par d√©faut
        if collection_name:
            collection = self.database_vect_service.get_or_create_collection(
                collection_name
            )
        else:
            collection = self.collection
        
        # 1. Embedder la question
        print(f"‚Üí Embedding de la question...")
        question_embedding = self.embedding_service.embed(
            texts=question,
            is_query=True,
            task_instruction="Given a question, retrieve passages that answer the question"
        )
        
        print(f"‚úì Question embedd√©e (dim={len(question_embedding[0])})")
        
        # 2. Recherche de similarit√© dans ChromaDB
        print(f"‚Üí Recherche des {top_k} documents les plus similaires...")
        results = self.database_vect_service.query(
            collection=collection,
            embedded_question=question_embedding,
            number_results=top_k
        )
        
        # Formater les r√©sultats
        formatted_results = self.database_vect_service.format_chroma_results(results)
        
        print(f"‚úì {len(formatted_results)} r√©sultats trouv√©s")
        
        # Afficher les distances
        for i, res in enumerate(formatted_results):
            print(f"  [{i+1}] Distance: {res['distance']:.4f} | Source: {res['metadata'].get('source', 'unknown')}")
        
        # 3. Construire le contexte pour le LLM
        context_items = []
        for item in formatted_results:
            context_items.append({
                "page_content": item["document"],
                "metadata": item["metadata"],
                "distance": item["distance"]
            })
        
        # 4. G√©n√©rer la r√©ponse avec le LLM
        print(f"‚Üí G√©n√©ration de la r√©ponse avec le LLM...")
        llm_response = self.llm_service.rag_completion(
            question=question,
            similarity_query=context_items
        )
        
        print(f"‚úì R√©ponse g√©n√©r√©e")
        
        # R√©sultat final
        result = {
            "question": question,
            "answer": llm_response,
            "context": formatted_results,
            "num_sources": len(formatted_results),
            "collection": collection.name
        }
        
        print(f"\n{'='*60}")
        print(f"R√âPONSE:")
        print(f"{'='*60}")
        print(llm_response)
        print(f"{'='*60}\n")
        
        return result
    
    def _convert_to_markdown(self, file_path: Path) -> str:
        """
        Convertit un fichier en Markdown en utilisant markitdown.
        
        Args:
            file_path: Chemin du fichier √† convertir
        
        Returns:
            str: Contenu Markdown
        """
        from markitdown import MarkItDown
        
        md_converter = MarkItDown()
        result = md_converter.convert(str(file_path))
        
        return result.text_content
    
    def list_collections(self):
        """Liste toutes les collections disponibles"""
        self.database_vect_service.get_list_collections()
    
    def delete_collection(self, collection_name: str):
        """Supprime une collection"""
        self.database_vect_service.delete_collection(collection_name)
        print(f"‚úì Collection '{collection_name}' supprim√©e")
    
    def get_collection_stats(self, collection_name: str = None) -> dict:
        """Obtient les statistiques d'une collection"""
        if collection_name:
            collection = self.database_vect_service.get_or_create_collection(
                collection_name
            )
        else:
            collection = self.collection
        
        all_data = collection.get()
        
        stats = {
            "collection_name": collection.name,
            "total_documents": len(all_data["ids"]),
            "sample_metadata": all_data["metadatas"][:3] if all_data["metadatas"] else []
        }
        
        return stats


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

if __name__ == "__main__":
    
    # Initialiser le service
    rag = RagService()
    
    # Exemple 1: Upload d'un PDF
    print("\n" + "="*80)
    print("EXEMPLE 1: Upload d'un PDF")
    print("="*80)
    
    # Chemin vers le PDF
    pdf_path = "ReferenceManualVision.pdf"
    
    # V√©rifier si le fichier existe
    if not Path(pdf_path).exists():
        print(f"‚ùå Erreur: Le fichier '{pdf_path}' n'existe pas!")
        print(f"üìÅ R√©pertoire actuel: {os.getcwd()}")
        print("\nüí° Assurez-vous que 'a.pdf' est dans le m√™me r√©pertoire que ce script.")
        exit(1)
    
    # Upload avec le chemin du PDF
    upload_result = rag.upload(
        file_path=pdf_path,
        chunk_size=1000,      # Chunks plus grands pour un PDF
        chunk_overlap=200
    )
    
    # Exemple 2: Poser une question sur le PDF
    print("\n" + "="*80)
    print("EXEMPLE 2: Poser une question sur le PDF")
    print("="*80)
    
    response = rag.ask(
        question="Code erreur 110",
        top_k=5,
    )
    
    print("\nüìä R√©sum√© de la r√©ponse:")
    print(f"  ‚Ä¢ Question: {response['question']}")
    print(f"  ‚Ä¢ Nombre de sources: {response['num_sources']}")
    print(f"  ‚Ä¢ Collection: {response['collection']}")
    
    if response['answer']:
        print(f"\nüí¨ R√©ponse du LLM:")
        print(response['answer'])
    else:
        print(f"\nüìÑ Contexte trouv√© (top 3):")
        for i, ctx in enumerate(response['context'][:3], 1):
            print(f"\n[{i}] Distance: {ctx['distance']:.4f}")
            print(f"    {ctx['document'][:200]}...")
    
    # Exemple 3: Statistiques de collection
    print("\n" + "="*80)
    print("EXEMPLE 3: Statistiques de la collection")
    print("="*80)
    
    stats = rag.get_collection_stats()
    print(f"\nüìä Collection: {stats['collection_name']}")
    print(f"üìö Total de chunks: {stats['total_documents']}")
    
    if stats['sample_metadata']:
        print(f"\nüìã Exemple de m√©tadonn√©es:")
        print(json.dumps(stats['sample_metadata'][0], indent=2, ensure_ascii=False))