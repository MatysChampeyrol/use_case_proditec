# ðŸ¤– POC Chatbot RAG - Use Case Proditec

Ce dÃ©pÃ´t contient une preuve de concept (POC) d'un **Assistant Documentaire Intelligent** basÃ© sur l'architecture **RAG (Retrieval-Augmented Generation)**. 

ConÃ§u dans le cadre du workshop **AI4Industry** pour le cas d'usage **Proditec**, cet outil permet d'interroger en langage naturel une base de connaissances constituÃ©e de documents techniques, manuels et rapports internes.

## ðŸš€ FonctionnalitÃ©s ClÃ©s

- **Interface Conversationnelle (Gradio)** : Chatbot intuitif pour poser des questions.
- **Support Multi-Formats** : Ingestion de fichiers PDF, DOCX, XLSX, PPIX, etc. CÃ´tÃ© backend
- **RAG Local & SÃ©curisÃ©** :
  - **Ollama** : Utilisation de LLM Open-Source Mistral en local.
  - **ChromaDB** : Base de donnÃ©es vectorielle persistante via Docker.
- **Transparence** : Citations prÃ©cises des sources et affichage des extraits utilisÃ©s pour gÃ©nÃ©rer chaque rÃ©ponse.
- **Outils de Traitement de DonnÃ©es** : Scripts avancÃ©s pour la conversion en masse et le nettoyage de documents (OCR, fusion de lignes brisÃ©es, suppression du bruit).

---

## ðŸ—ï¸ Architecture Technique

Le projet repose sur la stack technique suivante :

- **Frontend** : [Gradio](https://www.gradio.app/) (Interface Web).
- **Orchestration RAG** : [LangChain](https://www.langchain.com/).
- **LLM Engine** : [Ollama](https://ollama.com/) (pour l'infÃ©rence locale).
- **Vector Store** : [ChromaDB](https://www.trychroma.com/) (stockage des embeddings).
- **Conversion** : `MarkItDown` (Microsoft) pour la conversion universelle de documents vers Markdown.

---
## ModÃ¨les d'IA utilisÃ©s
ModÃ¨le LLM : mistral-7b
ModÃ¨le Embedding : intfloat/e5-mistral-7b-instruct

## ðŸ› ï¸ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© les Ã©lÃ©ments suivants :

1.  **Python 3.12**
2.  **Ollama** (installÃ© et fonctionnel sur votre machine)

> **Note** : Assurez-vous d'avoir tÃ©lÃ©chargÃ© le modÃ¨le dans Ollama au prÃ©alable : 

```bash
ollama pull mistral
```

---

## ðŸ“¦ Installation

### 1. Cloner le dÃ©pÃ´t

```bash
git clone https://github.com/MatysChampeyrol/use_case_proditec

cd use_case_proditec
```

### 2. CrÃ©er un environnement virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

## ðŸ’» Utilisation

Il existe deux faÃ§ons d'utiliser le projet : via l'interface web (recommandÃ©) ou via les scripts de traitement par lot.

### Option A : Interface Web (Chatbot)

Lancez l'application principale :

```bash
python -m src.main.run
```

L'interface sera accessible Ã  l'adresse **http://localhost:7860**.

**Ã‰tapes :**
1.  **Charger un document** : Dans le panneau de gauche, dÃ©posez un fichier (PDF, DOCX, etc.).
2.  **Configurer** (optionnel) : Ajustez la taille des "chunks" (morceaux de texte) et l'overlap.
3.  **Indexer** : Cliquez sur **ðŸš€ Indexer**. Le document est traitÃ© et ajoutÃ© Ã  ChromaDB.
4.  **Discuter** : Posez vos questions dans le chat Ã  droite. L'IA vous rÃ©pondra en citant ses sources.

---

## ðŸ“‚ Structure du Projet

```plaintext
use_case_proditec/
â”œâ”€â”€ docker-compose.yml       # Configuration Docker pour ChromaDB
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ convert_docs.py          # Script de conversion de documents (Batch)
â”œâ”€â”€ markdown_parser.py       # Script de nettoyage Markdown (Batch)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ run.py           # Point d'entrÃ©e de l'application (Gradio)
â”‚       â””â”€â”€ service/
â”‚           â””â”€â”€ rag_service.py # Logique mÃ©tier RAG (ingestion, requÃªtage)
â”œâ”€â”€ uploaded_docs/           # Dossier temporaire pour les uploads
â””â”€â”€ README.md                # Documentation du projet
```
